---
title: "Stacking"
author: "Alejandro Bolaños"
date: "2019-10-28"
version: 0.7
output: 
  html_document:
    theme: spacelab
    highlight: monochrome
    df_print: paged
#    toc: true
#    toc_depth: 2

vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

![](https://twistedsifter.files.wordpress.com/2013/01/art-of-rock-balancing-by-michael-grab-gravity-glue-3.jpg?w=498&h=750)


Nos adentraremos en esta clase, un paso adelante en el mundo de los ensambles, el temible stacking! Doloroso de ajustar, difícil de implementar y que suma poco... pero siempre recordamos que ese poco hizo ganar muchas competencias!

```{r}

rm( list=ls() )
gc()

```

Empezamos cargando un mes sobre el que vamos a entrenar y otro que usaremos de test. No utilizaremos mes de validación en esta ocasión. 

```{r}

library( "data.table" )
vsemillas <- c(810757,482071,340979,446441,917513)

periodo_delta <- function (foto_mes, delta) {
  
  len <-2
  
  step_cant <- delta
  
  step <- paste0(step_cant, " months")
  
  resultado <-  as.integer(strftime(seq(
    as.Date(paste0(foto_mes, "01"), format = "%Y%m%d"),
    length = len,
    by = step
  ), format = "%Y%m"))
  
  return(resultado[2])

  }

periodos_delta_serie <- function (foto_mes, delta) {
  sapply(0:(delta), periodo_delta, foto_mes=foto_mes)
}

cargar_meses <- function(foto_mes, diferencia) {
  dataset_list <- list()
  clases_vector <- c()
  
  for (fm in periodos_delta_serie(foto_mes, diferencia)) {
    dataset_mes <- readRDS(paste0("../datasets/dias/", fm, ".RDS"))

    clases <- ifelse(dataset_mes[, clase_ternaria] == "BAJA+2", 1, 0)
    dataset_mes[, clase_ternaria := NULL]
    dataset_mes[, numero_de_cliente := NULL]
    dataset_mes[, foto_mes := NULL]
    
    dataset_list[[as.character(fm)]] <- dataset_mes
    clases_vector <- c(clases_vector, clases)
  }
  resultado <- list()
  resultado[["datos"]] <- rbindlist(dataset_list)
  resultado[["clases"]] <- clases_vector
  resultado
}

p201802 <- cargar_meses(201802,0)
p201804 <- cargar_meses(201804,0)

```

El stacking es un ensemble que aplica un modelo (metamodelo) sobre la salida de otros modelos. Lo más importante es cómo lograr que el metamodelo aprenda sobre datos de test o validación y nunca sobre una salida de entrenamiento. Además tenemos que tener seguridad de que podemos medir correctamente la calidad de sus resultados.

Para esta empresa lo particionaremos el conjunto de entrenamiento en 5-fold, y estos siempre serán los mismos a través de todos los modelos que realicemos:

```{r}
library(caret)

set.seed(vsemillas[1])
p201802$folds <- createFolds(p201802$clases, k=5)

```

Modelaremos con la preciosa LightGBM, un XGBoost y un RF:

```{r}
library(lightgbm)
library(xgboost)
library(ranger)
library(randomForest)


modelar_lgbm <- function(ds,
                         target) {
  dtrain_lgm <- lgb.Dataset(data.matrix(ds), label = target)
  
  # Le damos una ayuda, con un conjunto de HP para que no quede la
  # ilusión que performace por debajo que el resto.
  
  params <-
    c(
      list(
        objective = "binary",
        num_iterations = 100,
        learning_rate = 0.025,
        num_leaves = 25,
        lambda_l1 = 0.6,
        min_gain_to_split = 0.85,
        max_bin = 31
      )
    )
  
  modelo <- lgb.train(params,
                      dtrain_lgm,
                      50,
                      verbose = -1)
  modelo
}

modelar_xgb <- function(ds,
           target
           ) {
    param = list(tree_method = "hist",
                        grow_policy = "lossguide")
    dtrain   <-
      xgb.DMatrix(data = data.matrix(ds),
                  label = target,
                  missing = NA)
    xgb.train(
      param,
      data = dtrain,
      nrounds = 50,
      missing = NA,
      maximize = TRUE,
      objective = "binary:logistic",
      verbose = FALSE
    )
}
modelar_ranger <- function(ds,
           target) {
  
  train <-  na.roughfix( ds )
  train$clase_binaria <- target
  variables <- round(sqrt(dim(ds)[2]-1))
  ranger( clase_binaria ~ ., data = train, 
                  probability=TRUE, 
                  num.trees=100,
                  min.node.size=10, 
                  mtry=variables,
                  splitrule="gini",
                  sample.fraction = 0.66,
                  importance = "impurity",
                  verbose=TRUE)	

}

```

Para hacer modelos `distintos`, vamos a usar los 3 anteriores sobre febrero del 2018. Suponemos igualmente, que estos son nuestros modelos estrella y queremos ensamblarlos, con lo cual, primero vamos a medir que tan buenos son dentro de dos meses:

```{r}

library(ROCR)

metricas <-
  function (probabilidades,
            clases,
            label,
            punto_corte = 0.025
            ) {
    
    roc <-  ROCR::prediction(probabilidades,
                             clases,
                             label.ordering = c(0, 1))
    auc <- unlist(ROCR::performance(roc, "auc")@y.values)
    
    dt_temp <- data.table(probabilidades, clases)
    
    dt_temp[, v := ifelse(clases == 1, 19500,-500)]
    
    ganancia <-
      dt_temp[probabilidades > punto_corte, sum(v)]
    data.table(label = label, ganancia = ganancia, auc = auc)
  }

resultados <- data.table()

m_lgbm <- modelar_lgbm(p201802$datos, p201802$clases)
p_lgbm <- predict(m_lgbm, data.matrix(p201804$datos), type = "prob")

m_xgb <- modelar_xgb(p201802$datos, p201802$clases)
p_xgb <- predict(m_xgb, data.matrix(p201804$datos), type = "prob")

m_rf   <- modelar_ranger(p201802$datos, p201802$clases)
test   <- na.roughfix(p201804$datos)
p_rf   <- predict(m_rf, test)$predictions[,2]

resultados <-
    rbindlist(list(
      metricas(p_lgbm, p201804$clases, "m_lgbm" ),
      metricas(p_xgb, p201804$clases, "m_xgb" ),
      metricas(p_rf, p201804$clases, "m_rf" )))

resultados
```

Teniendo de referencia estos números, pasamos a modelar nuestro stacking.

Haremos un `CV` dejando las probabilidades de validación en una nueva variable en un nuevo conjunto de datos:

```{r, eval=FALSE}

meta <- data.table()

for (f in 1:5) {
  
  train_folds <- c()
  test_fold <- c()
  
  for (j in 1:5) {
    if (f != j)
      train_folds <- c(train_folds, p201802$folds[[j]])
    else
      test_fold <- p201802$folds[[j]]
  }
  
  m_lgbm <- modelar_lgbm(p201802$datos[train_folds,],
                         p201802$clases[train_folds])
  p_lgbm <- predict(m_lgbm, 
                    data.matrix(p201802$datos[test_fold,])
                    , type = "prob")
  
  m_xgb <- modelar_xgb(p201802$datos[train_folds,],
                       p201802$clases[train_folds])
  p_xgb <- predict(m_xgb, 
                   data.matrix(p201802$datos[test_fold,]), 
                   type = "prob")
  
  m_rf   <- modelar_ranger(p201802$datos[train_folds,],
                           p201802$clases[train_folds])
  test   <- na.roughfix(p201802$datos[test_fold,])
  p_rf   <- predict(m_rf, test)$predictions[,2]

  test <-
    data.table(
          fold = f,
          test_fold,
          m_lgbm = p_lgbm,
          m_xgb  = p_xgb,
          m_rf   = p_rf)
  meta <- rbindlist(list(meta, test))
}

setorder(meta, test_fold)
meta$clases <- p201802$clases

# Agregamos algunas variables 
meta$mpasivos_margen <- p201802$datos$mpasivos_margen
meta$tmovimientos_ultimos90dias <- p201802$datos$tmovimientos_ultimos90dias
meta$mtarjeta_visa_consumo <- p201802$datos$mtarjeta_visa_consumo
meta$mcaja_ahorro_Paquete <- p201802$datos$mcaja_ahorro_Paquete

saveRDS(meta, "resultados/12_SK_res1.RDS")

```

```{r}

metadata <- readRDS("resultados/12_SK_res1.RDS")

```

Ya contamos con nuestro conjunto de entrenamiento, ahora vamos a probar en el mismo algunos modelos usando la misma estructura de `CV`.

Como son pocas variables, vamos a probar:

* Regresión Logística (sin y con variables extras) 
* XGB   (sin y con variables extras)

```{r}
meta_test <- data.table()

for (f in 1:5) {
  train_folds <- metadata$test_fold[metadata$fold != f]
  test_fold <- metadata$test_fold[metadata$fold == f]
  
  # Logística simple
  m_lr <-
    glm(clases ~  m_lgbm + m_xgb +  m_rf ,
        family = binomial(link = 'logit'),
        data = metadata[train_folds, ])
  
  p_lr <- predict(m_lr, metadata[test_fold, ], type = "response")
  
  # Logística con variables adicionales
  m_lr_w_var <-
    glm(
      clases ~ m_lgbm + m_xgb +  m_rf +
        mpasivos_margen + tmovimientos_ultimos90dias +
        mtarjeta_visa_consumo + mcaja_ahorro_Paquete,
      family = binomial(link = 'logit'),
      data = metadata[train_folds, ]
    )
  
  p_lr_w_var <-
    predict(m_lr_w_var, metadata[test_fold, ], type = "response")
  
  # XGB
  m_xgb <-
    modelar_xgb(metadata[train_folds, c("m_lgbm", "m_xgb", "m_rf"),
                          with = FALSE],
                 metadata[train_folds, clases])
  
  p_xgb <-
    predict(m_xgb, data.matrix(metadata[test_fold, 
                                       c("m_lgbm", "m_xgb", "m_rf"),
                                       with = FALSE]), type = "prob")
  
  # XGB con variables adicionales
  m_xgb_w_var <-
    modelar_xgb(metadata[train_folds, c(
      "m_lgbm", "m_xgb", "m_rf",
      "mpasivos_margen",
      "tmovimientos_ultimos90dias",
      "mtarjeta_visa_consumo" ,
      "mcaja_ahorro_Paquete"
    ), with = FALSE],
    metadata[train_folds, clases])
  
  p_xgb_w_var <-
    predict(m_xgb_w_var, data.matrix(metadata[test_fold, c(
      "m_lgbm", "m_xgb", "m_rf",
      "mpasivos_margen",
      "tmovimientos_ultimos90dias",
      "mtarjeta_visa_consumo" ,
      "mcaja_ahorro_Paquete"
    ), with = FALSE]), type = "prob")
  
  test <-
    data.table(
      fold = f,
      test_fold,
      m_lr = p_lr,
      m_lr_w_var = p_lr_w_var,
      m_xgb = p_xgb,
      m_xgb_w_var = p_xgb_w_var
    )
  meta_test <- rbindlist(list(meta_test, test))
}

setorder(meta_test, test_fold)
meta_test$clases <- p201802$clases

```

Pasamos a medir cuál es nuestro mejor modelo:

```{r}
rbindlist(list(
  metricas(meta_test$m_lr, meta_test$clases, "m_lr" ),
  metricas(meta_test$m_lr_w_var, meta_test$clases, "m_lr_w_var" ),
  metricas(meta_test$m_xgb, meta_test$clases, "m_xgb" ),
  metricas(meta_test$m_xgb_w_var, meta_test$clases, "m_xgb_w_var" )
)) 
```

Y por último estudiamos (y volvemos a medir) sobre un `OoT`.

Primero ajustamos sobre todo el conjunto de datos (recordemos que estamos trabajando en 201802):

```{r}

m_lr <-
  glm(clases ~  m_lgbm + m_xgb +  m_rf,
      family = binomial(link = 'logit'),
      data = metadata)

# Logística con variables adicionales
m_lr_w_var <-
  glm(
    clases ~  m_lgbm + m_xgb +  m_rf +
      mpasivos_margen + tmovimientos_ultimos90dias +
      mtarjeta_visa_consumo + mcaja_ahorro_Paquete,
    family = binomial(link = 'logit'),
    data = metadata
  )

m_xgb <-
  modelar_lgbm(metadata[, c("m_lgbm", "m_xgb", "m_rf"),
                        with = FALSE],
               metadata[, clases])

# XGB con variables adicionales
m_xgb_w_var <-
  modelar_lgbm(metadata[, c(
    "m_lgbm", "m_xgb", "m_rf",
    "mpasivos_margen",
    "tmovimientos_ultimos90dias",
    "mtarjeta_visa_consumo" ,
    "mcaja_ahorro_Paquete"
  ), with = FALSE],
  metadata[, clases])

```

Luego, necesitamos construir las variables de probabilidad sobre 201804 para nuestros modelos originales:

```{r}

meta_oot <- p201804$datos[, c(
  "mpasivos_margen",
  "tmovimientos_ultimos90dias",
  "mtarjeta_visa_consumo" ,
  "mcaja_ahorro_Paquete"
), with = FALSE]
meta_oot$clases <- p201804$clases

m_lgbm <- modelar_lgbm(p201802$datos, p201802$clases)
meta_oot$m_lgbm <- predict(m_lgbm, data.matrix(p201804$datos), type = "prob")

m_xgb2 <- modelar_xgb(p201802$datos, p201802$clases)
meta_oot$m_xgb <- predict(m_xgb2, data.matrix(p201804$datos), type = "prob")

m_rf   <- modelar_ranger(p201802$datos, p201802$clases)
test   <- na.roughfix(p201804$datos)
meta_oot$m_rf   <- predict(m_rf, test)$predictions[,2]
  
  

```

Ahora podemos aplicar la predicción de nuestros **Stackings**:

```{r}
meta_oot$m_lr <- predict(m_lr, meta_oot, type = "response")
  
meta_oot$m_lr_w_var <-
    predict(m_lr_w_var, meta_oot, type = "response")
  
meta_oot$m_sk_xgb <-
    predict(m_xgb, data.matrix(meta_oot[, c("m_lgbm", "m_xgb", "m_rf"),
                                       with = FALSE]), type = "prob")
  
meta_oot$m_sk_xgb_w_var <-
    predict(m_xgb_w_var, data.matrix(meta_oot[, c(
      "m_lgbm", "m_xgb", "m_rf",
      "mpasivos_margen",
      "tmovimientos_ultimos90dias",
      "mtarjeta_visa_consumo" ,
      "mcaja_ahorro_Paquete"
    ), with = FALSE]), type = "prob")
```

Y medimos en nuestro `OOT` todos los modelos:

```{r}

rbindlist(list(
  metricas(meta_oot$m_lgbm, meta_oot$clases, "m_lgbm" ),
  metricas(meta_oot$m_xgb, meta_oot$clases, "m_xgb" ),
  metricas(meta_oot$m_rf, meta_oot$clases, "m_rf" ),
  metricas(meta_oot$m_lr, meta_oot$clases, "m_lr" ),
  metricas(meta_oot$m_lr_w_var, meta_oot$clases, "m_lr_w_var" ),
  metricas(meta_oot$m_sk_xgb, meta_oot$clases, "m_sk_xgb" ),
  metricas(meta_oot$m_sk_xgb_w_var, meta_oot$clases, "m_sk_xgb_w_var" )
)) 

```

¿Observaciones? ¿Qué paso?

Terminemos con una prueba de sumar todas las variables del conjunto de datos y varios modelos a una parametrización:

```{r}
datos2 <- p201802$datos
datos2$m_xgb <- metadata$m_xgb
datos2$m_rf <- metadata$m_rf

datos2_test <- p201804$datos
datos2_test$m2 <- meta_oot$m_xgb
datos2_test$m3 <- meta_oot$m_rf

m2 <- modelar_lgbm(datos2, p201802$clases)
p2 <- predict(m2, data.matrix(datos2_test), type = "prob")


rbindlist(list(
  metricas(meta_oot$m_lgbm, meta_oot$clases, "m_lgbm" ),
  metricas(p2, meta_oot$clases, "lgbm + modelos" )))
```

Veremos que performance casi igual, muy apenas por encima.

Veamos la importancia de variables:

```{r}
imp <- lgb.importance(m2)
imp
```

Vemos que la información de los otros modelos son variables que al estar muy correlacionadas con el target son las mejores.

* Y si usamos varias de las mejores parametrizaciones de una búsqueda de parámetros?

Es una interesante pregunta con una interesante respuesta. Lo invito a probarlo y utilice la siguiente exploración para entender los resultados:

```{r}
cor(meta_oot[,c("m_xgb","m_lgbm","m_rf"), with=FALSE], method = 'spearman')
```

* como se pueden llevar esta metodología usando un conjunto de validación en un mes futuro.
