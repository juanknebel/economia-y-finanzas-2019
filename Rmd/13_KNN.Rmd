---
title: "kNN"
author: "Alejandro Bolaños"
date: "2019-11-07"
version: 0.7
output: 
  html_document:
    theme: spacelab
    highlight: monochrome
    df_print: paged
#    toc: true
#    toc_depth: 2

vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

> Dime con quién andas y te dire quién eres

Hoy veremos rápidamente y sin demasiado detalle uno de los algoritmos de clasificación más simples. 

## k-NN: k-nearest neighbors

El funcionamiento es simple. Si tenemos `k = 3`, para cada caso en nuestro conjunto de `test` buscamos los 3 elementos del conjunto de `train` que tienen menor distancia (usualmente la euclídea) con nuestro caso. Miramos las clases de estos 3 últimos, y la clase predominante es la que se le asigna a nuestro elemento de `test`.

Podemos notar dos pequeños detalles:

* **NO** tiene un proceso de ajuste. No es algo necesariamente bueno, el proceso de scoreo puede ser muy ineficiente. 

* Si tenemos un `k=3`,y tratamos de asignar una clase a un elemento, donde un caso del conjunto de entrenamiento está casi pegado y tiene una clase `A`, y los 2 restantes, se encuentran a una distancia considerable y son de una clase `B`, con la definición básica nos clasificaría a nuestro elemento de tipo `B`, algo que sería cuestionable. 

Para evitar este problema, se suele utilizar `weighted k-NN`, que se puede obtener con la siguiente ecuación:

$$ P(x_i \in y_j) = \displaystyle\frac{\displaystyle\sum\limits_{k=1}^K \left( \frac{1}{d_{ik}}\cdot(n_{ik} \in y_j) \right)}{\displaystyle\sum\limits_{k=1}^K \left( \frac{1}{d_{ik}} \right)} $$

Lo que devuelve es un valor entre 0 y 1. Que ese valor entre 0 y 1 sea una probabilidad es algo que dejaré a la imaginación del alumno. 

k-NN es especialmente usado en:

* Sistemas de recomendación
* Clasificación de imagenes - face recognition 
* Prototipos - caso especial de k=1

Donde no suele verse mucho su uso es en el dominio de los modelos de propensión. Frente a las árboles, logísticas, ensamble de árboles y redes neuronales, la presencia de este algoritmo es muy bajo. 

Pero veamos que podemos esperar de un `k-NN` en nuestro conjunto de datos:


```{r}

rm( list=ls() )
gc()

```

Trabajaremos con los meses 201802 para entrenar y 201804 para testear: 

```{r}
library( "data.table" )
vsemillas <- c(810757,482071,340979,446441,917513)

periodo_delta <- function (foto_mes, delta) {
  
  len <-2
  
  step_cant <- delta
  
  step <- paste0(step_cant, " months")
  
  resultado <-  as.integer(strftime(seq(
    as.Date(paste0(foto_mes, "01"), format = "%Y%m%d"),
    length = len,
    by = step
  ), format = "%Y%m"))
  
  return(resultado[2])

  }

periodos_delta_serie <- function (foto_mes, delta) {
  sapply(0:(delta), periodo_delta, foto_mes=foto_mes)
}

cargar_meses <- function(foto_mes, diferencia=0, baja1=FALSE) {
  dataset_list <- list()
  clases_vector <- c()
  
  for (fm in periodos_delta_serie(foto_mes, diferencia)) {
    #dataset_mes <- fread(paste0("~/dias/", fm, "_dias.txt"))
    dataset_mes <- readRDS(paste0("../datasets/dias/", fm, ".RDS"))

    if (baja1) {
      clases <- ifelse(dataset_mes[, clase_ternaria] == "CONTINUA", 0, 1)
    } else {
      clases <- ifelse(dataset_mes[, clase_ternaria] == "BAJA+2", 1, 0)
    }
    dataset_mes[, clase_ternaria := NULL]
    dataset_mes[, numero_de_cliente := NULL]
    dataset_mes[, foto_mes := NULL]
    
    dataset_list[[as.character(fm)]] <- dataset_mes
    clases_vector <- c(clases_vector, clases)
  }
  resultado <- list()
  resultado[["datos"]] <- rbindlist(dataset_list)
  resultado[["clases"]] <- clases_vector
  resultado
}

p201802 <- cargar_meses(201802,0)
p201804 <- cargar_meses(201804,0)

```

Lo más importante de todo, buscar una buena librería que nos ahorre tener que lidiar con las pésimas librerías de R que trae por defecto. Sin buscar mucho, aparece la librería `fastknn`

```{r}
#library("devtools")
#install_github("davpinto/fastknn")

library("fastknn")
```

Algo importante además es tomar tan solo una muestra de los datos de entrenamiento, ya que tomar todos tiene un costo computacional muy alto:

```{r}
set.seed(vsemillas[1])
tomar_muestra <- function(clases, resto=2500 ) {
      t <- clases == 1
      r <- rep(FALSE, length(clases))
      r[!t][sample.int(resto,n=(length(clases)-sum(clases)))] <- TRUE
      t | r
}
s <- tomar_muestra(p201802$clases)

```

Y sin preocuparme por mucho más, tiro el primer modelo, con un `k=5` elegido sin ningún motivo importante. 

```{r, error=TRUE}
knn.out1 <- fastknn(xtr = data.matrix(p201802$datos[s,]), 
                   ytr = factor(p201802$clases[s]), 
                   xte = data.matrix(p201804$datos), k = 5)
```

Apa! no le gustan los NA. ¿Por qué no?

Ok, vamos a imputar los NA a cero. Vemos que las variables con NA están asociadas a tarjetas de créditos. ¿Podemos suponer que los mismos se deben a `left join` sin casos en la "right table"? 

Deberíamos mirarlos con más tiempo, para imputarlos de la mejor forma, pero haciendo todo `Quick & Dirty`, vamos a imputar todos con cero:

```{r}

p201802$datos[is.na(p201802$datos)] <- 0
p201804$datos[is.na(p201804$datos)] <- 0

```

Y volvemos a ejecutar

```{r}
t0 <- Sys.time()
knn.out1 <- fastknn(xtr = data.matrix(p201802$datos[s,]), 
                   ytr = factor(p201802$clases[s]), 
                   xte = data.matrix(p201804$datos), k = 5)
t1 <- Sys.time()
t1-t0
```

Y vamos a medir con nuestra ya maltratada función de `métricas`

```{r}

library(ROCR)

metricas <-
  function (probabilidades,
            clases,
            punto_corte = 0.025,
            curva_ganancia = TRUE,
            rango_curva = c(0.002, 0.15),
            paso_curva = 0.001) {
    resultado <- list()
    
    
    roc <-  ROCR::prediction(probabilidades,
                             clases,
                             label.ordering = c(0, 1))
    resultado$auc <- unlist(ROCR::performance(roc, "auc")@y.values)
    
    dt_temp <- data.table(probabilidades, clases)
    
    dt_temp[, v := ifelse(clases == 1, 19500,-500)]
    
    resultado$captura <-
      dt_temp[probabilidades > punto_corte, sum(clases)] / dt_temp[, sum(clases)]
    
    resultado$ganancia <-
      dt_temp[probabilidades > punto_corte, sum(v)]
    
    if (curva_ganancia) {
      dt_temp2 <- dt_temp[, .(gan = sum(v)), by = probabilidades]
      
      setorder(dt_temp2, -probabilidades)
      
      dt_temp2[, ganancia := cumsum(gan)]
      
      resultado$curva$max_ganancia <-
        dt_temp2[max(ganancia) == ganancia, ganancia]
      
      resultado$curva$mejor_punto_corte <-
        dt_temp2[max(ganancia) == ganancia, probabilidades]
      
      x <-
        seq(from = rango_curva[1],
            to = rango_curva[2],
            by = paso_curva)
      y <-
        sapply(x, function(x)
          dt_temp[probabilidades > x, sum(v)])
      
      resultado$curva$graph <- data.table(x, y)
      
      
      resultado$curva$captura <-
        dt_temp[probabilidades > resultado$curva$mejor_punto_corte, sum(clases)] /
        dt_temp[, sum(clases)]
      
    }
    
    resultado
  }

```

Y nos da...

```{r}
knn.out1.m <- metricas(knn.out1$prob[,2], p201804$clases)

knn.out1.m$ganancia
```

Ouch! 
Wait... veamos la curva de ganancia:

```{r}

library(ggplot2)
knn.out1.m <- metricas(knn.out1$prob[,2], p201804$clases,
                       curva_ganancia = TRUE,
                       rango_curva = c(0.01, 1),
                       paso_curva = 0.01)

ggplot(knn.out1.m$curva$graph, aes(x,y)) + geom_line()

```

No es lo que estamos acostumbrados a ver. ¿Tendrá algo que ver las "probabilidades" que devuelve nuestro algoritmo?

Vamos a tomar un decisión más mala que buena. Para no entrar en detalle en el punto de corte, usaremos de ahora en más como métrica comparativa la `máxima ganancia posible`. Si, se pierde riqueza y se puede sobreestimar un modelo, pero... acaso no es lo mismo que se hace con la métrica **KS**?

.. también usaremos el `auc`

Luego:

```{r}
knn.out1.m$curva$max_ganancia
```


```{r}
knn.out1.m$auc
```

La verdad es que me asombré cuando ví estos resultados, con una área bajo la curva de 0.832 y con esperable ganancia superior a 5MM, por solamente ejecutar este algoritmo que hace poco más que usar la regla de medir... me pareció muy interesante!

Como somos gente cultivada en el mundo del analytics, conocemos _algo_ nuestros datos, recordamos

```{r}
ggplot(p201802$datos, aes(tmovimientos_ultimos90dias)) + geom_histogram()
```

Y pensamos que esa asimetría y esos outliers nos deben esta _complicando_ la calidad del modelo. Deberíamos transformar estas variables en formas más amables para nuestro algoritmo.

Uno puede aplicar estrategicamente las funciones adecuadas hasta encontrar una transformación que `normalice` los datos o ... usar una librería que haga este trabajo por nosotros:

[Using the bestNormalize Package](https://cran.r-project.org/web/packages/bestNormalize/vignettes/bestNormalize.html)

Apliquemos en la variable anterior:

```{r}
library(bestNormalize)

bn <- bestNormalize(p201802$datos$tmovimientos_ultimos90dias,
                    allow_orderNorm = FALSE, 
                    out_of_sample = FALSE)

print(bn$chosen_transform)
```
```{r}

ggplot(as.data.frame(bn$x.t), aes(bn$x.t)) + geom_histogram()

```

Otro importante problema que soluciona es el de las escalas:

* Si en la fórmula de distancia entran la variable `edad` y también la variable `sueldo`, la presencia de la primera se va a ver fuertemente disminuida.

Aplicamos a todas las variables las mejores transformaciones. Con el no menor detalle, que determinar cuál es la mejor transformación se debe realizar únicamente con los datos de entrenamiento y luego, aplicarlos a `test` 

```{r, warning=FALSE}

train <- p201802$datos
test <- p201804$datos

for (v in colnames(p201802$datos)) {
  bn <- bestNormalize(p201802$datos[[v]], 
                      allow_orderNorm = FALSE, 
                      out_of_sample = FALSE) 
  train[[v]] <- bn$x.t
  test[[v]] <- predict(bn, test[[v]])

}
```

Se bloque la salida de los warning para que no devuelva una gran cantidad de mensajes, pero de no haberlo hecho, veríamos que varias de las transformaciones generaron valor `NA`.

No haremos mucho más que imputarlas en cero, y volver a probar nuestro modelo:

```{r}

train[is.na(train)] <- 0
test[is.na(test)] <- 0

t0 <- Sys.time()
knn.out2 <- fastknn(xtr = data.matrix(train[s,]), 
                   ytr = factor(p201802$clases[s]), 
                   xte = data.matrix(test), k = 5)
t1 <- Sys.time()
t1-t0

knn.out2.m <- metricas(knn.out2$prob[,2], p201804$clases)

knn.out2.m$auc
knn.out2.m$curva$max_ganancia

```

Wow! Subió casi un 4% el `auc` y sustancialmente la ganancia máxima posible! 

A quien se le hubiera ocurrido que el **feature engineering** era útil. Tan solo se repitió en clase un centenar de veces...


Las transformaciones que están pueden ser utilizadas por otros algoritmos que son sensibles a los mismos problemas que `k-NN`.
 
Veamos de aplicar una mejora en la transformación para que no se produzcan tantos `NA`.

```{r}

# Volvemos a cargar, porque imputamos y perdimos los valores originales.
p201802 <- cargar_meses(201802,0)
p201804 <- cargar_meses(201804,0)

variables <- colnames(p201802$datos)

train <- p201802$datos
test <- p201804$datos

# Sacamos las variables que no tienen información
unarias <- c()
for (v in variables) {
  if (length(unique(p201802$datos[[v]])) == 1) {
    unarias <- c(unarias, v)
  }
}

variables <- setdiff(variables, unarias)

# Dejamos a las binarias en paz
binarias <- c()
for (v in variables) {
  if (length(unique(p201802$datos[[v]])) == 2 ) { 
    binarias <- c(binarias, v)
  }
}


for (v in setdiff(variables,binarias)) {
  bn <- bestNormalize(p201802$datos[[v]], 
                      allow_orderNorm = FALSE, 
                      out_of_sample = FALSE,
                      warn = FALSE)  
  train[[v]] <- bn$x.t
  test[[v]] <- predict(bn, test[[v]])
}

# Finalmente imputamos porque algo nos quedo
train[is.na(train)] <- 0
test[is.na(test)] <- 0

```

Y probamos si con este _pequeño_ esfuerzo más mejora nuestro `k-NN`

```{r}
t0 <- Sys.time()
knn.out3 <- fastknn(xtr = data.matrix(train[s,]), 
                   ytr = factor(p201802$clases[s]), 
                   xte = data.matrix(test), k = 5)
t1 <- Sys.time()
t1-t0

knn.out3.m <- metricas(knn.out3$prob[,2], p201804$clases)

knn.out3.m$auc
knn.out3.m$curva$max_ganancia
```

Vemos que logramos solucionar varios de los problemas, y con la solución logramos mejorar un poco nuestros resultados.

Nos adentramos en la hiperparametrización de este algoritmo no-paramétrico (?). Los parámetros que podríamos alterar son:

* **Distancia**: Hasta ahora usamos la euclídea, pero no es la única que nos puede ser útil. 
* **Método de clasificación**: Puede ser la votación simple, donde cada vecino tiene un voto, y la ponderada (la que estuvimos utilizando), donde se pondera la cercanía del vecino en el voto. 
* **k**: El más importante de los parámetros, el que determina cuál es la cantidad de vecinos más cercanos que tienen que participar en la clasificación de un nuevo elemento. Podemos buscar este parámetro a través de un `Grid Search`. Podemos usar ser un poco más inteligente y usar una `búsqueda bayesiana`, o podemos percatarnos que si calculamos la matriz de distancia para la máxima cantidad de `k` que seteemos, podemos usar la misma para calcular el valor de todos los `k` anteriores.

La librería `fastknn` no devuelve la matriz de distancias, sin embargo, si miramos el código de esta librería, vemos que utiliza otra (RANN que a su vez usa ANN escrita en C++) que si nos devuelve la matriz que necesitamos. La invaluable belleza del Open Source.

```{r}

t0 <- Sys.time()
knn.dist100 <- RANN::nn2(data = data.matrix(train[s,]), 
                         query = data.matrix(test), 
                         k = 60, treetype = 'kd',
                         searchtype = 'standard')
t1 <- Sys.time()
t1 - t0

```

  NOTA: Los kd-tree son estructuras arboladas que permiten una búsqueda más eficiente de los vecinos más cercanos. Primero se toma todos los elementos de conjunto de entrenamiento y arma el árbol en función de su ubicación espacial. Luego los elementos de prueba avanzan por el último, evitando comparaciones innecesarias. Se pone en duda que para la cantidad de dimensiones que contamos, el algoritmo kd-tree ofrezca grandes mejores en frente de la comparación de todos contra todos.

Vemos que no tarda mucho. Nos resta armar la función que calcule las "probabilidades" del `k-NN`. Tomaremos prestado parte de la librería `fastknn`:

```{r}

knn <- function(k, dist, clases_train, clases_test) {
  
  label.mat <- matrix(factor(clases_train)[dist$nn.idx], ncol = 60)
  r <- sapply(levels(factor(clases_train)), function(cl, d, y) {
            rowSums(1/d * (y == cl)) / rowSums(1/d)
         }, d = pmax(dist$nn.dists, 1e-15)[,1:k], y = label.mat[,1:k], 
         simplify=FALSE, USE.NAMES=TRUE)
  r <- as.matrix(do.call('cbind.data.frame', r))
  r <- sweep(r, 1, rowSums(r), "/")
  m <- metricas(r[,2],clases_test)
  data.table(K = k, ganancia = m$curva$max_ganancia, auc = m$auc)
  
}

```

Y aplicamos para `2 <= k <= 60`:

```{r}
resultados <- data.table()

for (i in 2:60) {
  r <- knn(i, knn.dist100, p201802$clases[s], p201804$clases)
  resultados <- rbindlist(list(resultados, r))
}

```

Por último graficamos para ver cuál es el mejor `k` en esta configuración para la máxima ganancia:

```{r}

ggplot(resultados, aes(K, ganancia)) + geom_line()

```

Donde el valor máximo es:

```{r}

resultados[ganancia == max(ganancia), ]

```

Y repetimos lo mismo para `auc`:

```{r}
ggplot(resultados, aes(K, auc)) + geom_line()
```
```{r}
resultados[auc == max(auc), ]
```

Lo que nos coincide, para ambos un `k = 45`

Observaciones finales

* No es un mal modelo, es mejor de lo que esperaba para el esfuerzo dedicado. 

* Queda un camino largo para poder obtener los mejores resultados:

  * La influencia de la muestra de entrenamiento es muy importante, e hicimos poco sobre. Hay que evaular la estabilidad entre muestras.
  * Una vez más observamos la importancia de las transformaciones, podemos hacer mucho más en este punto. Recordando que es útil también para otros algoritmos.
  * Falta probar la inclusión de los `BAJA+1`. 
  * Falta armar un conjunto de train con información de más meses.
  * No hicimos nada para evitar la **Maldición de la dimensionalidad**: ¿Qué algoritmos aplicarían?

* La librería fastknn trae algunas utilidades como su propia reducción de dimensiones, que pueden ser útiles para la entrada para otros algoritmos.

* ¿Podrá `k-NN` puede ser un buen complemento de otros algoritmos usando stacking?

